# adopted from "A Survey of Monte Carlo Tree Search Methods"
# by Browne et. al (2012)

# the following is pseudo-code for the upper confidance bound
# for trees algorithm (UCT) 




# s0 is the start state 
function UCT_SEARCH(s0) 
	create root node v0 with state s0
	while (within computational budget) do

		# using the tree policy, find a node of interest vl
		vl    <-- TREE_POLICY(v0)

		# delta is the reward from a playout from vl using the 
		# default policy
		delta <-- DEFAULT_POLICY(s(vl))

		# propagate the result back up through the tree
		BACK_PROPAGATE(vl, delta)

	return a(BEST_CHILD(v0, 0))




# this function takes some node v of the tree and looks at its
# descendants and selects the most promising node on which to 
# perform a rollout 
function TREE_POLICY(v)
	while (v is nonterminal) do
		if (v not fully expanded) then
			return EXPAND(v)
		else
			v <-- BEST_CHILD(v, Cp)
	return v 



# this selects the best child according to the UCT equation 
# (i.e., treating the next selection as a multi-armed bandit
# problem)
function BEST_CHILD(v, c)
	return arg max (v' in children of v) {
	           (Q(v') / N(v')) + c * sqrt( 2ln(N(v)) / N(v') )
	       }




# this is the policy with which we perform a simulation from state 
# s to the end of the game, returning the reward from that rollout 
function DEFAULT_POLICY(s)
	while (s is non-terminal) do
		# let A(s) be all possible actions from state s
		choose a in A(s) uniformly at random

		# what is f?
		s <-- f(s, a)

	return reward for state s (i.e. delta)




# 
function BACK_PROPAGATE(v, delta)
	while (v is not null) do
		N(v) <-- N(v) + 1
		Q(v) <-- Q(v) + delta(v, p)
		v    <-- parent of v 



